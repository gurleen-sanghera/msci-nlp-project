{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Best Model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import necessary modules\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "import numpy as np\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "X-ROkFOASmN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the data\n",
        "train_h = pd.read_csv('train_stances.csv')\n",
        "train_b = pd.read_csv('train_bodies.csv')\n",
        "test_h = pd.read_csv('competition_test_stances.csv')\n",
        "test_b = pd.read_csv('competition_test_bodies.csv')"
      ],
      "metadata": {
        "id": "Jn8PygQ2SoWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to merge the headlines and articlebody datasets\n",
        "def merge(d1, d2):\n",
        "  data = pd.merge(d1, d2, how='inner', left_on=['Body ID'], right_on=['Body ID'])\n",
        "  return data"
      ],
      "metadata": {
        "id": "OhOdJeJKSp0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing specials chars, puting words in lowercase\n",
        "def clean(s):\n",
        "  return re.sub(\"[^a-zA-Z]\", \" \",str(s)).lower()\n",
        "\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "# lemmatizing\n",
        "def normalize_word(w):\n",
        "  return _wnl.lemmatize(w).lower()\n",
        "\n",
        "# tokenizing\n",
        "def get_tokenized_lemmas(s):\n",
        "  return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "# removing stopwords\n",
        "def remove_stopwords(l):\n",
        "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "# do all the above preprocessing and put back into sentences\n",
        "def preprocess(data, title):\n",
        "  content = []\n",
        "  content = [clean(line) for line in data[title]]\n",
        "  content = [remove_stopwords(line) for line in data[title]]\n",
        "  content = [get_tokenized_lemmas(line) for line in data[title]]\n",
        "  content = [' '.join(x) for x in content]\n",
        "  data[title] = content\n"
      ],
      "metadata": {
        "id": "xoSFBaGaSrjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do all necessary preprocessing on all data we need\n",
        "preprocess(train_h, 'Headline')\n",
        "preprocess(train_b, 'articleBody')\n",
        "preprocess(test_h, 'Headline')\n",
        "preprocess(test_b, 'articleBody')\n"
      ],
      "metadata": {
        "id": "NtB0PEonStnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge bodies and headlines\n",
        "test_data = merge(test_h, test_b)\n",
        "train_data = merge(train_h, train_b)"
      ],
      "metadata": {
        "id": "wHe41KhGSv47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get list of all sentences from the headlines and the bodies to use in the tf vectorizer\n",
        "sentences = []\n",
        "for line in train_data['Headline']:\n",
        "  sentences.append(line)\n",
        "for line in train_data['articleBody']:\n",
        "  sentences.append(line)"
      ],
      "metadata": {
        "id": "TFaW8sw1Sxxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function vectorizes the words/sentences and also measures the similaries between headings and bodies\n",
        "# it returns data pertaining to the vectorized haedings, bodies, and their similarities\n",
        "def tf_data(df):\n",
        "    tfvectorizer = TfidfVectorizer(max_features=2000, use_idf=False).fit(sentences)\n",
        "    tfidfvectorizer = TfidfVectorizer(max_features=2000, use_idf=True).fit(sentences)\n",
        "    data = []\n",
        "    for index, row in df.iterrows():\n",
        "        head = row['Headline']\n",
        "        body = row['articleBody']\n",
        "        tf_head = tfvectorizer.transform([head]).toarray().reshape(1, -1)\n",
        "        tf_body = tfvectorizer.transform([body]).toarray().reshape(1, -1)\n",
        "        head_tfidf = tfidfvectorizer.transform([head]).toarray()\n",
        "        body_tfidf = tfidfvectorizer.transform([body]).toarray()\n",
        "        # using cosine similarity\n",
        "        tfidf_cos = cosine_similarity(head_tfidf, body_tfidf).reshape(1, -1)\n",
        "        # merging the features\n",
        "        features= np.squeeze(np.c_[tf_head, tf_body, tfidf_cos])\n",
        "        data.append(features)\n",
        "    data = np.array(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "EGvp-OclSz_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf_data(train_data)\n",
        "X_test = tf_data(test_data)"
      ],
      "metadata": {
        "id": "ndanv2QLS2Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace the stances with numerical values\n",
        "train_data.Stance.replace('unrelated', 1, True)\n",
        "train_data.Stance.replace('agree', 2, True)\n",
        "train_data.Stance.replace('disagree', 3, True)\n",
        "train_data.Stance.replace('discuss', 4, True)\n",
        "\n",
        "y_train = train_data['Stance']"
      ],
      "metadata": {
        "id": "pFKKmd7aS3-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "prediction = model.predict(X_test)"
      ],
      "metadata": {
        "id": "7q6vsooPS63z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stances = []\n",
        "for p in range(len(prediction)):\n",
        "  if prediction[p] == 1: \n",
        "    stances.append(\"unrelated\")\n",
        "  if prediction[p] == 2: \n",
        "    stances.append(\"disagree\")\n",
        "  if prediction[p] == 3: \n",
        "    stances.append(\"agree\")\n",
        "  if prediction[p] == 4: \n",
        "    stances.append(\"discuss\")"
      ],
      "metadata": {
        "id": "iQoWYT0US-yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(real, test):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i in range(len(real)):\n",
        "    if real[i] == test[i]:\n",
        "      correct += 1\n",
        "    total += 1\n",
        "  print( correct/total)"
      ],
      "metadata": {
        "id": "Be3cWgq1TCav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df = {}\n",
        "predictions_df = pd.DataFrame({'Stance': stances})\n",
        "\n",
        "get_accuracy(test_data['Stance'], predictions_df['Stance'])\n"
      ],
      "metadata": {
        "id": "FgFwS9IVTEt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_submission(gold_labels, test_labels):\n",
        "    score = 0.0\n",
        "\n",
        "    for i in range(len(gold_labels)):  \n",
        "      if gold_labels[i] == test_labels[i]:\n",
        "        if gold_labels[i] == 'unrelated':\n",
        "          score += 0.25\n",
        "        if gold_labels[i]!= 'unrelated':\n",
        "          score += 0.75\n",
        "      elif gold_labels[i] != 'unrelated':\n",
        "        if test_labels[i] in ['agrees', 'disagrees', 'discusses']:\n",
        "          score += 0.25\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "sRVGeQJkTIko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get score from this model\n",
        "score_submission(test_data['Stance'], predictions_df['Stance'])"
      ],
      "metadata": {
        "id": "Bm68t_7fTKQY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}