{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM + Dropouts",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nMh5vUE4429"
      },
      "outputs": [],
      "source": [
        "# importing required libraries\n",
        "import sys\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Activation\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from string import punctuation\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting values for these parameters\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 128\n",
        "MAX_VOCAB_SIZE = 40000\n",
        "MAX_SENT_LEN = 150\n",
        "N_EPOCHS = 10\n",
        "LSTM_DIM = 100"
      ],
      "metadata": {
        "id": "-Hj7cNtK48s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "# W2V_DIR = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "#\n",
        "#W2V_DIR = 'glove.twitter.27B.50d.txt'\n",
        "\n",
        "W2V_DIR = 'GoogleNews-vectors-negative300.bin.gz'"
      ],
      "metadata": {
        "id": "MeM4prqtU6kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9dbd08-4182-498f-a555-297019d41ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading in the train and test data\n",
        "train_stance = pd.read_csv('train_stances.csv')\n",
        "train_body = pd.read_csv('train_bodies.csv')\n",
        "test_stance = pd.read_csv('test_stances_unlabeled.csv')\n",
        "test_body = pd.read_csv('competition_test_bodies.csv')\n",
        "\n",
        "# replacing the stances with numerical values so that a model can be trained on them\n",
        "train_stance.replace('unrelated', 1, True)\n",
        "train_stance.replace('agree', 2, True)\n",
        "train_stance.replace('disagree', 3, True)\n",
        "train_stance.replace('discuss', 4, True)\n",
        "\n",
        "# merging datasets so that the bodies and titles can be together\n",
        "df_train = train_stance.join(train_body.set_index('Body ID'), on='Body ID')\n",
        "df_test = test_stance.join(test_body.set_index('Body ID'), on='Body ID')\n"
      ],
      "metadata": {
        "id": "njZt4wqr5IP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7tYZOWTwHFoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "from sklearn import feature_extraction\n",
        "from tqdm import tqdm\n",
        "\n",
        "def clean(s):\n",
        "  return re.sub(\"[^a-zA-Z]\", \" \",str(s)).lower()\n",
        "\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "def normalize_word(w):\n",
        "  return _wnl.lemmatize(w).lower()\n",
        "\n",
        "def get_tokenized_lemmas(s):\n",
        "  return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "def remove_stopwords(l):\n",
        "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "def preprocess(data, title):\n",
        "  content = []\n",
        "  content = [clean(line) for line in data[title]]\n",
        "  content = [remove_stopwords(line) for line in data[title]]\n",
        "  content = [get_tokenized_lemmas(line) for line in data[title]]\n",
        "  content = [' '.join(x) for x in content]\n",
        "  data[title] = content\n"
      ],
      "metadata": {
        "id": "Zg9l1mRX9_Yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a841f5ad-13bb-4929-ecf2-784eb08f9c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do all necessary preprocessing on all data we need\n",
        "preprocess(train_stance, 'Headline')\n",
        "preprocess(train_body, 'articleBody')\n",
        "preprocess(test_stance, 'Headline')\n",
        "preprocess(test_body, 'articleBody')\n"
      ],
      "metadata": {
        "id": "XwMO3RYyLg4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to merge the headlines and articlebody datasets\n",
        "def merge(d1, d2):\n",
        "  data = pd.merge(d1, d2, how='inner', left_on=['Body ID'], right_on=['Body ID'])\n",
        "  return data"
      ],
      "metadata": {
        "id": "9xlZAmeoLZCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge bodies and headlines\n",
        "test_data = merge(test_stance, test_body)\n",
        "train_data = merge(train_stance, train_body)"
      ],
      "metadata": {
        "id": "6vtg7KDJM1Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the words sequences\n",
        "wsh_train = [text_to_word_sequence(text) for text in train_data['Headline']]\n",
        "wsb_train = [text_to_word_sequence(text) for text in train_data['articleBody']]\n",
        "wsh_test = [text_to_word_sequence(text) for text in df_test['Headline']]\n",
        "wsb_test = [text_to_word_sequence(text) for text in df_test['articleBody']]\n",
        "\n",
        "seq = []\n",
        "for i in range(len(wsh_train)):\n",
        "    seq.append(wsh_train[i])\n",
        "for i in range(len(wsb_train)):\n",
        "    seq.append(wsb_train[i])\n",
        "for i in range(len(wsh_test)):\n",
        "    seq.append(wsh_test[i])\n",
        "for i in range(len(wsb_test)):\n",
        "    seq.append(wsb_test[i])\n"
      ],
      "metadata": {
        "id": "Cqsx4e7JLQD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#special_chars = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "# tokenizing and converting text to numerical values\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([seq for seq in seq])\n",
        "\n",
        "ws_train = [list(line) for line in wsh_train]\n",
        "for line in range(len(wsh_train)):\n",
        "    ws_train[line].extend(wsb_train[line])\n",
        "\n",
        "ws_test = [list(line) for line in wsh_test]\n",
        "for line in range(len(wsh_test)):\n",
        "    ws_test[line].extend(wsb_test[line])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in ws_train])"
      ],
      "metadata": {
        "id": "TMeMMYWMBN_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in ws_test])\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "_2-wXYpPBbIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the target variable (stance)\n",
        "y_train = df_train['Stance']\n",
        "LabelEncoder = LabelEncoder()\n",
        "LabelEncoder.fit(y_train)\n",
        "train_encode = LabelEncoder.transform(y_train)\n",
        "# one hot encoding\n",
        "y_train = np_utils.to_categorical(train_encode)"
      ],
      "metadata": {
        "id": "vfsxivua6QKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(W2V_DIR, binary=True, limit=50000)\n",
        "\n",
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(\n",
        "len(tokenizer.word_index) + 1, EMBEDDING_DIM)) \n",
        "for word, i in tokenizer.word_index.items(): \n",
        "    try:\n",
        "        embeddings_vector = embeddings[word]\n",
        "    except KeyError:\n",
        "        embeddings_vector = None\n",
        "\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix[i] = embeddings_vector\n",
        "\n",
        "del embeddings"
      ],
      "metadata": {
        "id": "gTK4VwNYQY4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential Model\n",
        "import time\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                      output_dim=EMBEDDING_DIM,\n",
        "                      weights=[embeddings_matrix], trainable=True, name='word_embedding_layer', \n",
        "                      mask_zero=True))\n",
        "model.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer'))\n",
        "model.add(Dropout(rate=0.8, name='dropout_1'))\n",
        "model.add(Dense(4, activation='softmax', name='output_layer'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "starting_time = time.time()\n",
        "model.fit(X_train, y_train,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        epochs=N_EPOCHS\n",
        "                        )\n",
        "training_time = round((time.time() - starting_time) /60, 2)"
      ],
      "metadata": {
        "id": "OqRx7kLdG7qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd51fe45-42e9-4780-e119-2e8271c31380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 366s 923ms/step - loss: 0.8403 - accuracy: 0.7283\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 324s 830ms/step - loss: 0.8123 - accuracy: 0.7313\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 313s 801ms/step - loss: 0.8027 - accuracy: 0.7313\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 300s 768ms/step - loss: 0.7889 - accuracy: 0.7312\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 283s 724ms/step - loss: 0.7753 - accuracy: 0.7314\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 279s 714ms/step - loss: 0.7630 - accuracy: 0.7315\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 288s 737ms/step - loss: 0.7508 - accuracy: 0.7320\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 298s 763ms/step - loss: 0.7406 - accuracy: 0.7334\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 310s 794ms/step - loss: 0.7302 - accuracy: 0.7346\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 305s 780ms/step - loss: 0.7208 - accuracy: 0.7366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "oTvTPtPTeZxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8e054b-14bb-4aa5-82ae-914de93db8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " word_embedding_layer (Embed  (None, None, 300)        9754200   \n",
            " ding)                                                           \n",
            "                                                                 \n",
            " lstm_layer (LSTM)           (None, 100)               160400    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 4)                 404       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,915,004\n",
            "Trainable params: 9,915,004\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model so can be accessed again without running\n",
        "model.save('LSTM')"
      ],
      "metadata": {
        "id": "0TDw0FcCNzWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7757d14-691b-4cba-e133-671c378f6b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: LSTM/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: LSTM/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fb854c8b8d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_time)"
      ],
      "metadata": {
        "id": "jrQLOdBkEx_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb675758-2ff1-419e-ebba-7748f1e82da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = keras.models.load_model('LSTM')"
      ],
      "metadata": {
        "id": "VSRj7_zqO1pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions from model\n",
        "predictions = new_model.predict(X_test)"
      ],
      "metadata": {
        "id": "8E-RSUdvPNWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the value closest to \"1\" in every entry is the largest value\n",
        "# the value closest to \"1\" sits in the index corresponding to the stance\n",
        "# the following gets the stances per entry, but in integer form\n",
        "stance_integer = [np.argmax(p, axis = -1) for p in predictions]\n",
        "\n",
        "for s in range(len(stance_integer)):\n",
        "  if stance_integer[s] == 0: \n",
        "    stance_integer[s] = \"unrelated\"\n",
        "  if stance_integer[s] == 1: \n",
        "    stance_integer[s] = \"disagree\"\n",
        "  if stance_integer[s] == 2: \n",
        "    stance_integer[s] = \"agree\"\n",
        "  if stance_integer[s] == 3: \n",
        "    stance_integer[s] = \"discuss\"\n",
        "\n",
        "predictions_df = {}\n",
        "predictions_df = pd.DataFrame({'Stance': stance_integer})"
      ],
      "metadata": {
        "id": "Qd73sqCjPxtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "competition_test_stance = pd.read_csv('competition_test_stances.csv')"
      ],
      "metadata": {
        "id": "PsJJ53d-Up5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(real, test):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i in range(len(real)):\n",
        "    if real[i] == test[i]:\n",
        "      correct += 1\n",
        "    total += 1\n",
        "  print( correct/total)\n",
        "\n",
        "get_accuracy(competition_test_stance['Stance'], predictions_df['Stance'])"
      ],
      "metadata": {
        "id": "rYqZs0UlWZh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe180bd-63d4-414c-f57f-955a37405ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7094794003069296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score_submission(gold_labels, test_labels):\n",
        "    score = 0.0\n",
        "\n",
        "    for i in range(len(gold_labels)):  \n",
        "      if gold_labels[i] == test_labels[i]:\n",
        "        if gold_labels[i] == 'unrelated':\n",
        "          score += 0.25\n",
        "        if gold_labels[i]!= 'unrelated':\n",
        "          score += 0.75\n",
        "      elif gold_labels[i] != 'unrelated':\n",
        "        if test_labels[i] in ['agrees', 'disagrees', 'discusses']:\n",
        "          score += 0.25\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "HG-YeENEeAuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get score from this model\n",
        "\n",
        "score_submission(competition_test_stance['Stance'], predictions_df['Stance'])\n"
      ],
      "metadata": {
        "id": "2VTirs_VeE8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31328334-fd2d-4a75-8d5f-f0e71e6b76f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4533.5"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## adding drop outs\n",
        "import time\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer', \n",
        "                          mask_zero=True))\n",
        "\n",
        "model.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer')) ## can add Bidirectional in here too\n",
        "model.add(Dropout(rate=0.8, name='dropout1'))\n",
        "model.add(Dense(4, activation='softmax', name='activation1'))\n",
        "\n",
        "model.add(Dropout(rate=0.5, name='dropout2'))\n",
        "model.add(Activation(activation='relu', name='activation2'))\n",
        "\n",
        "model.add(Dense(4, activation='softmax', name='output_layer2'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "starting_time = time.time()\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS\n",
        "         )\n",
        "training_time = round((time.time() - starting_time) /60, 2)"
      ],
      "metadata": {
        "id": "jkcCgNuaYVfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7443a82-9b29-474f-82da-e1b319814a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " word_embedding_layer (Embed  (None, None, 300)        9754200   \n",
            " ding)                                                           \n",
            "                                                                 \n",
            " lstm_layer (LSTM)           (None, 100)               160400    \n",
            "                                                                 \n",
            " dropout1 (Dropout)          (None, 100)               0         \n",
            "                                                                 \n",
            " activation1 (Dense)         (None, 4)                 404       \n",
            "                                                                 \n",
            " dropout2 (Dropout)          (None, 4)                 0         \n",
            "                                                                 \n",
            " activation2 (Activation)    (None, 4)                 0         \n",
            "                                                                 \n",
            " output_layer2 (Dense)       (None, 4)                 20        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,915,024\n",
            "Trainable params: 160,824\n",
            "Non-trainable params: 9,754,200\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 219s 549ms/step - loss: 1.0263 - accuracy: 0.7234\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 216s 552ms/step - loss: 0.8864 - accuracy: 0.7313\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 215s 551ms/step - loss: 0.8468 - accuracy: 0.7313\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 209s 534ms/step - loss: 0.8276 - accuracy: 0.7313\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 214s 546ms/step - loss: 0.8164 - accuracy: 0.7313\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 217s 554ms/step - loss: 0.8104 - accuracy: 0.7313\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 212s 542ms/step - loss: 0.8041 - accuracy: 0.7313\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 210s 538ms/step - loss: 0.8023 - accuracy: 0.7313\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 210s 538ms/step - loss: 0.8013 - accuracy: 0.7313\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 210s 536ms/step - loss: 0.8000 - accuracy: 0.7313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions2 = model.predict(X_test)"
      ],
      "metadata": {
        "id": "OcoIZ1-SjMb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta0DLwQZptpf",
        "outputId": "1a71a6e8-bbbc-4efa-c7f7-316536d1231c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stance_integer = [np.argmax(p, axis = -1) for p in predictions2]\n",
        "\n",
        "for s in range(len(stance_integer)):\n",
        "  if stance_integer[s] == 0: \n",
        "    stance_integer[s] = \"unrelated\"\n",
        "  if stance_integer[s] == 1: \n",
        "    stance_integer[s] = \"disagree\"\n",
        "  if stance_integer[s] == 2: \n",
        "    stance_integer[s] = \"agree\"\n",
        "  if stance_integer[s] == 3: \n",
        "    stance_integer[s] = \"discuss\"\n",
        "\n",
        "predictions_df = {}\n",
        "predictions_df = pd.DataFrame({'Stance': stance_integer})\n"
      ],
      "metadata": {
        "id": "H8m4_KXLjsk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get accuracy\n",
        "get_accuracy(competition_test_stance['Stance'], predictions_df['Stance'])"
      ],
      "metadata": {
        "id": "l0qcfPjtj0pB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0976987a-7392-4457-f42b-456fddd2d709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7220320308503522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get score from this model\n",
        "\n",
        "score_submission(competition_test_stance['Stance'], predictions_df['Stance'])\n"
      ],
      "metadata": {
        "id": "UQVLunAJeL2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f656413-12c1-460a-ca30-ca244e3115f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4587.25"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ]
}