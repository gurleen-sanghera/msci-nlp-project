{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM + Dropouts",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3nMh5vUE4429"
      },
      "outputs": [],
      "source": [
        "# importing required libraries\n",
        "import sys\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Activation\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from string import punctuation\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting values for these parameters\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 128\n",
        "MAX_VOCAB_SIZE = 40000\n",
        "MAX_SENT_LEN = 150\n",
        "N_EPOCHS = 10\n",
        "LSTM_DIM = 100"
      ],
      "metadata": {
        "id": "-Hj7cNtK48s_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading in the train and test data\n",
        "train_stance = pd.read_csv('train_stances.csv')\n",
        "train_body = pd.read_csv('train_bodies.csv')\n",
        "test_stance = pd.read_csv('test_stances_unlabeled.csv')\n",
        "test_body = pd.read_csv('competition_test_bodies.csv')\n",
        "\n",
        "# replacing the stances with numerical values so that a model can be trained on them\n",
        "train_stance.replace('unrelated', 1, True)\n",
        "train_stance.replace('agree', 2, True)\n",
        "train_stance.replace('disagree', 3, True)\n",
        "train_stance.replace('discuss', 4, True)\n",
        "\n",
        "# merging datasets so that the bodies and titles can be together\n",
        "df_train = train_stance.join(train_body.set_index('Body ID'), on='Body ID')\n",
        "df_test = test_stance.join(test_body.set_index('Body ID'), on='Body ID')\n"
      ],
      "metadata": {
        "id": "njZt4wqr5IP0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn import feature_extraction\n",
        "from tqdm import tqdm\n",
        "\n",
        "def clean(s):\n",
        "  return re.sub(\"[^a-zA-Z]\", \" \",str(s)).lower()\n",
        "\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "def normalize_word(w):\n",
        "  return _wnl.lemmatize(w).lower()\n",
        "\n",
        "def get_tokenized_lemmas(s):\n",
        "  return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "def remove_stopwords(l):\n",
        "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "def preprocess(data, title):\n",
        "  content = []\n",
        "  content = [clean(line) for line in data[title]]\n",
        "  content = [remove_stopwords(line) for line in data[title]]\n",
        "  content = [get_tokenized_lemmas(line) for line in data[title]]\n",
        "  content = [' '.join(x) for x in content]\n",
        "  data[title] = content\n"
      ],
      "metadata": {
        "id": "Zg9l1mRX9_Yg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do all necessary preprocessing on all data we need\n",
        "preprocess(train_stance, 'Headline')\n",
        "preprocess(train_body, 'articleBody')\n",
        "preprocess(test_stance, 'Headline')\n",
        "preprocess(test_body, 'articleBody')\n"
      ],
      "metadata": {
        "id": "XwMO3RYyLg4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "d41d859c-59f3-471c-a6b9-0c0ef43922d9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-cd060156a1b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do all necessary preprocessing on all data we need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_stance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Headline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'articleBody'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_stance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Headline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'articleBody'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-29c315cf31c1>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(data, title)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_tokenized_lemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-29c315cf31c1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_tokenized_lemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-29c315cf31c1>\u001b[0m in \u001b[0;36mget_tokenized_lemmas\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_tokenized_lemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormalize_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to merge the headlines and articlebody datasets\n",
        "def merge(d1, d2):\n",
        "  data = pd.merge(d1, d2, how='inner', left_on=['Body ID'], right_on=['Body ID'])\n",
        "  return data"
      ],
      "metadata": {
        "id": "9xlZAmeoLZCq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge bodies and headlines\n",
        "test_data = merge(test_stance, test_body)\n",
        "train_data = merge(train_stance, train_body)"
      ],
      "metadata": {
        "id": "6vtg7KDJM1Kp"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the words sequences\n",
        "wsh_train = [text_to_word_sequence(text) for text in train_data['Headline']]\n",
        "wsb_train = [text_to_word_sequence(text) for text in train_data['articleBody']]\n",
        "wsh_test = [text_to_word_sequence(text) for text in df_test['Headline']]\n",
        "wsb_test = [text_to_word_sequence(text) for text in df_test['articleBody']]\n",
        "\n",
        "seq = []\n",
        "for i in range(len(wsh_train)):\n",
        "    seq.append(wsh_train[i])\n",
        "for i in range(len(wsb_train)):\n",
        "    seq.append(wsb_train[i])\n",
        "for i in range(len(wsh_test)):\n",
        "    seq.append(wsh_test[i])\n",
        "for i in range(len(wsb_test)):\n",
        "    seq.append(wsb_test[i])\n"
      ],
      "metadata": {
        "id": "Cqsx4e7JLQD4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "special_chars = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "# tokenizing and converting text to numerical values\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=special_chars)\n",
        "tokenizer.fit_on_texts([seq for seq in seq])\n",
        "\n",
        "ws_train = [list(line) for line in wsh_train]\n",
        "for line in range(len(wsh_train)):\n",
        "    ws_train[line].extend(wsb_train[line])\n",
        "\n",
        "ws_test = [list(line) for line in wsh_test]\n",
        "for line in range(len(wsh_test)):\n",
        "    ws_test[line].extend(wsb_test[line])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in ws_train])"
      ],
      "metadata": {
        "id": "TMeMMYWMBN_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in ws_test])\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "_2-wXYpPBbIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the target variable (stance)\n",
        "y_train = df_train['Stance']\n",
        "LabelEncoder = LabelEncoder()\n",
        "LabelEncoder.fit(y_train)\n",
        "train_encode = LabelEncoder.transform(y_train)\n",
        "# one hot encoding\n",
        "y_train = np_utils.to_categorical(train_encode)"
      ],
      "metadata": {
        "id": "vfsxivua6QKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(W2V_DIR, binary=True, limit=50000)\n",
        "\n",
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(\n",
        "len(tokenizer.word_index) + 1, EMBEDDING_DIM)) \n",
        "for word, i in tokenizer.word_index.items(): \n",
        "    try:\n",
        "        embeddings_vector = embeddings[word]\n",
        "    except KeyError:\n",
        "        embeddings_vector = None\n",
        "\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix[i] = embeddings_vector\n",
        "\n",
        "del embeddings"
      ],
      "metadata": {
        "id": "gTK4VwNYQY4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential Model\n",
        "import time\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                      output_dim=EMBEDDING_DIM,\n",
        "                      weights=[embeddings_matrix], trainable=True, name='word_embedding_layer', \n",
        "                      mask_zero=True))\n",
        "model.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer'))\n",
        "model.add(Dropout(rate=0.8, name='dropout_1'))\n",
        "model.add(Dense(4, activation='softmax', name='output_layer'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "starting_time = time.time()\n",
        "model.fit(X_train, y_train,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        epochs=N_EPOCHS\n",
        "                        )\n",
        "training_time = round((time.time() - starting_time) /60, 2)"
      ],
      "metadata": {
        "id": "OqRx7kLdG7qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eedb898-3ffe-44b7-e8df-9f72df9f577d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 305s 769ms/step - loss: 0.7198 - accuracy: 0.7542\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 299s 764ms/step - loss: 0.5576 - accuracy: 0.8028\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 290s 742ms/step - loss: 0.5095 - accuracy: 0.8147\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 290s 742ms/step - loss: 0.4849 - accuracy: 0.8209\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 291s 744ms/step - loss: 0.4629 - accuracy: 0.8264\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 296s 756ms/step - loss: 0.4489 - accuracy: 0.8291\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 295s 755ms/step - loss: 0.4365 - accuracy: 0.8312\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 333s 853ms/step - loss: 0.4224 - accuracy: 0.8361\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 309s 790ms/step - loss: 0.4158 - accuracy: 0.8373\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 293s 748ms/step - loss: 0.4057 - accuracy: 0.8404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model so can be accessed again without running\n",
        "model.save('LSTM')"
      ],
      "metadata": {
        "id": "0TDw0FcCNzWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0459de-a82b-4176-a476-7e90207723ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: LSTM/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: LSTM/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f73773db9d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrQLOdBkEx_r",
        "outputId": "d96e1bc6-a519-4df0-b33e-5363f8b2c884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = keras.models.load_model('LSTM')"
      ],
      "metadata": {
        "id": "VSRj7_zqO1pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions from model\n",
        "predictions = new_model.predict(X_test)"
      ],
      "metadata": {
        "id": "8E-RSUdvPNWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the value closest to \"1\" in every entry is the largest value\n",
        "# the value closest to \"1\" sits in the index corresponding to the stance\n",
        "# the following gets the stances per entry, but in integer form\n",
        "stance_integer = [np.argmax(p, axis = -1) for p in predictions]\n",
        "\n",
        "for s in range(len(stance_integer)):\n",
        "  if stance_integer[s] == 0: \n",
        "    stance_integer[s] = \"unrelated\"\n",
        "  if stance_integer[s] == 1: \n",
        "    stance_integer[s] = \"disagree\"\n",
        "  if stance_integer[s] == 2: \n",
        "    stance_integer[s] = \"agree\"\n",
        "  if stance_integer[s] == 3: \n",
        "    stance_integer[s] = \"discuss\"\n",
        "\n",
        "predictions_df = {}\n",
        "predictions_df = pd.DataFrame({'Stance': stance_integer})"
      ],
      "metadata": {
        "id": "Qd73sqCjPxtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "competition_test_stance = pd.read_csv('competition_test_stances.csv')"
      ],
      "metadata": {
        "id": "PsJJ53d-Up5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(real, test):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i in range(len(real)):\n",
        "    if real[i] == test[i]:\n",
        "      correct += 1\n",
        "    total += 1\n",
        "  print( correct/total)\n",
        "\n",
        "get_accuracy(competition_test_stance['Stance'], predictions_df['Stance'])"
      ],
      "metadata": {
        "id": "rYqZs0UlWZh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae50664-ab5b-4f2f-ad28-f2f6baae5fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5600283319560855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## adding drop outs\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer', \n",
        "                          mask_zero=True))\n",
        "\n",
        "model.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer')) ## can add Bidirectional in here too\n",
        "model.add(Dropout(rate=0.8, name='dropout1'))\n",
        "model.add(Dense(4, activation='softmax', name='activation1'))\n",
        "\n",
        "model.add(Dropout(rate=0.5, name='dropout2'))\n",
        "model.add(Activation(activation='relu', name='activation2'))\n",
        "\n",
        "model.add(Dense(4, activation='softmax', name='output_layer2'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS\n",
        "         )"
      ],
      "metadata": {
        "id": "jkcCgNuaYVfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a9b6e7-eaa2-4b6e-fec6-7679409061ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " word_embedding_layer (Embed  (None, None, 300)        10793700  \n",
            " ding)                                                           \n",
            "                                                                 \n",
            " lstm_layer (LSTM)           (None, 100)               160400    \n",
            "                                                                 \n",
            " dropout1 (Dropout)          (None, 100)               0         \n",
            "                                                                 \n",
            " activation1 (Dense)         (None, 4)                 404       \n",
            "                                                                 \n",
            " dropout2 (Dropout)          (None, 4)                 0         \n",
            "                                                                 \n",
            " activation2 (Activation)    (None, 4)                 0         \n",
            "                                                                 \n",
            " output_layer2 (Dense)       (None, 4)                 20        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,954,524\n",
            "Trainable params: 160,824\n",
            "Non-trainable params: 10,793,700\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 204s 510ms/step - loss: 1.0380 - accuracy: 0.7046\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 197s 505ms/step - loss: 0.9004 - accuracy: 0.7313\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 198s 506ms/step - loss: 0.8577 - accuracy: 0.7313\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 198s 505ms/step - loss: 0.8283 - accuracy: 0.7312\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 197s 504ms/step - loss: 0.8106 - accuracy: 0.7313\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 197s 504ms/step - loss: 0.8096 - accuracy: 0.7310\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 197s 504ms/step - loss: 0.8049 - accuracy: 0.7313\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 206s 527ms/step - loss: 0.8030 - accuracy: 0.7313\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 198s 506ms/step - loss: 0.7968 - accuracy: 0.7313\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 205s 524ms/step - loss: 0.7974 - accuracy: 0.7313\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7376601ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions2 = model.predict(X_test)"
      ],
      "metadata": {
        "id": "OcoIZ1-SjMb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stance_integer = [np.argmax(p, axis = -1) for p in predictions2]\n",
        "\n",
        "for s in range(len(stance_integer)):\n",
        "  if stance_integer[s] == 0: \n",
        "    stance_integer[s] = \"unrelated\"\n",
        "  if stance_integer[s] == 1: \n",
        "    stance_integer[s] = \"disagree\"\n",
        "  if stance_integer[s] == 2: \n",
        "    stance_integer[s] = \"agree\"\n",
        "  if stance_integer[s] == 3: \n",
        "    stance_integer[s] = \"discuss\"\n",
        "\n",
        "predictions_df = {}\n",
        "predictions_df = pd.DataFrame({'Stance': stance_integer})\n"
      ],
      "metadata": {
        "id": "H8m4_KXLjsk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(competition_test_stance['Stance'], predictions_df['Stance'])"
      ],
      "metadata": {
        "id": "l0qcfPjtj0pB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29520cee-2851-4a0b-c63d-468f3ee989e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7220320308503522\n"
          ]
        }
      ]
    }
  ]
}